"""
äº¤ä»˜ç‰©æ™ºèƒ½ä½“ä¸»å®šä¹‰
"""
from collections.abc import Callable

from langchain.agents import create_agent
from langchain.agents.middleware import ModelRequest, ModelResponse, dynamic_prompt, wrap_model_call

from src import config
from src.agents.common import BaseAgent, load_chat_model, BaseContext
from src.agents.common.middlewares import context_based_model, logging_middleware
from src.utils.logging_config import logger
from .context import DeliverableContext
from .prompts import DELIVERABLE_SYSTEM_PROMPT, OUTLINE_GENERATION_PROMPT, DRAFT_GENERATION_PROMPT, POLISH_PROMPT
from .tools import get_tools


@dynamic_prompt
def deliverable_prompt_middleware(request: ModelRequest) -> str:
    """åŠ¨æ€æ³¨å…¥äº¤ä»˜ç‰©ç”Ÿæˆçš„ç³»ç»Ÿæç¤ºè¯å’Œä¸Šä¸‹æ–‡

    åŠŸèƒ½ï¼š
    1. æ³¨å…¥åŸºç¡€ç³»ç»Ÿæç¤ºè¯
    2. æ ¹æ®åœºæ™¯æ³¨å…¥ä¸“ç”¨æç¤ºè¯ï¼ˆoutline/draft/polishï¼‰
    3. æ ¹æ®ä¸Šä¸‹æ–‡æ³¨å…¥é¡¹ç›®ä¿¡æ¯ã€çŸ¥è¯†åº“å†…å®¹
    4. åœ¨å±€éƒ¨æ¨¡å¼ä¸‹æ³¨å…¥å½“å‰ç« èŠ‚ä¿¡æ¯
    """
    context = request.runtime.context
    
    # è·å–æ™ºèƒ½ä½“åç§°ï¼Œç”¨äºæ—¥å¿—
    agent_name = getattr(request.runtime.context, "name", "DeliverableAgent")
    
    # ä»æ¶ˆæ¯å†å²ä¸­è·å–æœ€æ–°çš„ç”¨æˆ·è¾“å…¥ä½œä¸ºæ£€æµ‹å‚è€ƒ
    query = ""
    if request.messages:
        for m in reversed(request.messages):
            role = m.get("role") if isinstance(m, dict) else getattr(m, "type", None)
            if role == "user":
                query = m.get("content") if isinstance(m, dict) else getattr(m, "content", "")
                break

    # ğŸ†• æ™ºèƒ½åœºæ™¯æ£€æµ‹ï¼šå¦‚æœ context ä¸­æ²¡æœ‰ scenarioï¼Œåˆ™è‡ªåŠ¨æ£€æµ‹
    from .config import detect_scenario
    scenario = getattr(context, 'scenario', None)
    if not scenario:
        scenario = detect_scenario(context, query=query)
        # è®°å½•æ£€æµ‹åˆ°çš„åœºæ™¯
        logger.bind(
            type="business_node",
            node="scenario_detection",
            scenario=scenario,
            projectId=getattr(context, "projectId", ""),
            deliverableId=getattr(context, "deliverableId", "")
        ).info("[{}] >>> è‡ªåŠ¨æ£€æµ‹åˆ°åœºæ™¯: {}", agent_name, scenario)
    
    logger.bind(
        type="business_node",
        node="request_start",
        scenario=scenario,
        projectId=getattr(context, "projectId", ""),
        deliverableId=getattr(context, "deliverableId", "")
    ).info("[{}] >>> å¼€å§‹å¤„ç†è¯·æ±‚, Scenario: {}", agent_name, scenario)

    # åŸºç¡€æç¤ºè¯
    full_prompt = DELIVERABLE_SYSTEM_PROMPT

    # ğŸ†• æ ¹æ®åœºæ™¯æ³¨å…¥ä¸“ç”¨æç¤ºè¯
    if scenario:
        scenario = scenario.lower()

        if scenario == 'outline':
            full_prompt += f"\n\n### å¤§çº²ç”Ÿæˆä¸“é¡¹æŒ‡å¯¼\n{OUTLINE_GENERATION_PROMPT}"

        elif scenario == 'draft':
            full_prompt += f"\n\n### åˆç¨¿ç”Ÿæˆä¸“é¡¹æŒ‡å¯¼\n{DRAFT_GENERATION_PROMPT}"

        elif scenario == 'polish':
            full_prompt += f"\n\n### å…¨æ–‡æ¶¦è‰²ä¸“é¡¹æŒ‡å¯¼\n{POLISH_PROMPT}"

        # writing åœºæ™¯ä¸éœ€è¦é¢å¤–æç¤ºè¯ï¼Œä½¿ç”¨åŸºç¡€æç¤ºè¯å³å¯
        pass

    # æ³¨å…¥é¡¹ç›®ä¸Šä¸‹æ–‡ï¼ˆå¦‚æœæœ‰ï¼‰
    if hasattr(context, 'projectContext') and context.projectContext:
        project_info = []
        if context.projectContext.get('industry'):
            project_info.append(f"è¡Œä¸šé¢†åŸŸï¼š{context.projectContext['industry']}")
        if context.projectContext.get('tech_stack'):
            project_info.append(f"æŠ€æœ¯æ ˆï¼š{', '.join(context.projectContext['tech_stack'])}")
        if context.projectContext.get('business_domain'):
            project_info.append(f"ä¸šåŠ¡é¢†åŸŸï¼š{context.projectContext['business_domain']}")

        if project_info:
            full_prompt += f"\n\n### é¡¹ç›®èƒŒæ™¯\n{'ï¼›'.join(project_info)}"

    # æ³¨å…¥çŸ¥è¯†åº“ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
    if hasattr(context, 'kb_files') and context.kb_files:
        kb_count = len(context.kb_files)
        full_prompt += f"\n\nçŸ¥è¯†åº“åŒ…å« {kb_count} ä¸ªæ–‡ä»¶ï¼Œè¯·å……åˆ†åˆ©ç”¨è¿™äº›æ–‡ä»¶çš„å†…å®¹ç”Ÿæˆä¸“ä¸šå¤§çº²ã€‚"

    # æ³¨å…¥äº¤ä»˜ç‰©ç±»å‹ï¼ˆå¦‚æœæœ‰ï¼‰
    if hasattr(context, 'deliverableType') and context.deliverableType:
        full_prompt += f"\n\näº¤ä»˜ç‰©ç±»å‹ï¼š{context.deliverableType}"

    # æ³¨å…¥ç›®æ ‡å­—æ•°å’Œæ–‡æ¡£è§„æ¨¡ï¼ˆå¦‚æœæœ‰ï¼‰
    if hasattr(context, 'targetWords') and context.targetWords:
        total_words = context.targetWords
        # ç¡®å®šæ–‡æ¡£è§„æ¨¡
        if total_words >= 100000:
            scale = "è¶…å¤§å‹æ–‡æ¡£ï¼ˆâ‰¥10ä¸‡å­—ï¼Œæœ€å¤§4çº§ï¼‰"
        elif total_words >= 50000:
            scale = "å¤§å‹æ–‡æ¡£ï¼ˆ5-10ä¸‡å­—ï¼Œæœ€å¤§3-4çº§ï¼‰"
        elif total_words >= 10000:
            scale = "ä¸­å‹æ–‡æ¡£ï¼ˆ1-5ä¸‡å­—ï¼Œæœ€å¤§2-3çº§ï¼‰"
        else:
            scale = "å°å‹æ–‡æ¡£ï¼ˆ<1ä¸‡å­—ï¼Œæœ€å¤§2çº§ï¼‰"

        full_prompt += f"\n\n### æ–‡æ¡£è§„æ¨¡ä¿¡æ¯"
        full_prompt += f"\n- ç›®æ ‡æ€»å­—æ•°ï¼š{total_words} å­—"
        full_prompt += f"\n- æ–‡æ¡£è§„æ¨¡ï¼š{scale}"
        full_prompt += f"\n\nè¯·æ ¹æ®ä¸Šè¿°æ–‡æ¡£è§„æ¨¡ï¼Œä¸¥æ ¼æŒ‰ç…§'æ™ºèƒ½å±‚çº§ç”Ÿæˆè§„åˆ™'ä¸­å¯¹åº”è§„æ¨¡çš„å±•å¼€è§„åˆ™æ‰§è¡Œã€‚"

    # æ³¨å…¥åŠ¨æ€ä¸Šä¸‹æ–‡ï¼ˆå¦‚å½“å‰ç« èŠ‚ã€å­—æ•°é™åˆ¶ç­‰ï¼‰
    active_section = None
    if hasattr(context, 'documentStructure') and context.documentStructure:
        active_section = next(
            (s for s in context.documentStructure
             if str(s.get('id')) == str(getattr(context, 'activeSectionId', ''))),
            None
        )

    if active_section:
        full_prompt += f"\n\nå½“å‰æ­£åœ¨åä½œç« èŠ‚ï¼š{active_section.get('title')}"
        if active_section.get('targetWords'):
            full_prompt += f"\næœ¬ç« èŠ‚ç›®æ ‡å­—æ•°ï¼š{active_section.get('targetWords')}å­—"

    # æ³¨å…¥ç”¨æˆ·ä¼ å…¥çš„é¢å¤–ç³»ç»Ÿæç¤ºï¼ˆå¦‚é£æ ¼é”šå®šï¼‰
    if hasattr(context, 'system_prompt') and context.system_prompt:
        full_prompt += f"\n\né¢å¤–çº¦æŸæ¡ä»¶ï¼š\n{context.system_prompt}"

    return full_prompt


@wrap_model_call
async def deliverable_model_middleware(request: ModelRequest, handler: Callable) -> ModelResponse:
    """åŠ¨æ€è°ƒæ•´æ¨¡å‹å‚æ•°ä¸­é—´ä»¶

    æ ¹æ®å½“å‰åœºæ™¯ï¼ˆScenarioï¼‰åŠ¨æ€è°ƒæ•´æ¨¡å‹å‚æ•°ï¼ˆtemperature, top_p, max_tokens ç­‰ï¼‰
    """
    context = request.runtime.context
    
    # è·å–æ™ºèƒ½ä½“åç§°ï¼Œç”¨äºæ—¥å¿—
    agent_name = getattr(request.runtime.context, "name", "DeliverableAgent")
    
    # è·å–åœºæ™¯
    from .config import detect_scenario, get_scenario_params
    scenario = getattr(context, 'scenario', None)
    if not scenario:
        # ä»æ¶ˆæ¯å†å²ä¸­è·å–æœ€æ–°çš„ç”¨æˆ·è¾“å…¥ä½œä¸ºæ£€æµ‹å‚è€ƒ
        query = ""
        if request.messages:
            for m in reversed(request.messages):
                role = m.get("role") if isinstance(m, dict) else getattr(m, "type", None)
                if role == "user":
                    query = m.get("content") if isinstance(m, dict) else getattr(m, "content", "")
                    break
        scenario = detect_scenario(context, query=query)
        
    # è·å–åœºæ™¯å¯¹åº”çš„å‚æ•°
    scenario_params = get_scenario_params(scenario)
    
    # åŠ¨æ€è°ƒæ•´æ¨¡å‹å‚æ•°
    # ä¼˜å…ˆçº§ï¼šcontext ä¸­æ˜¾å¼è®¾ç½®çš„å‚æ•° > åœºæ™¯é»˜è®¤å‚æ•°
    # æ³¨æ„ï¼šBaseContext çš„é»˜è®¤å€¼ï¼ˆ0.7/0.9ï¼‰å¯èƒ½ä¼šè¦†ç›–åœºæ™¯å‚æ•°ï¼Œè¿™é‡Œéœ€è¦å¤„ç†
    
    def get_param(name, default_val):
        # å¦‚æœ context ä¸­æœ‰è¯¥å­—æ®µï¼Œä¸”ä¸æ˜¯ None
        val = getattr(context, name, None)
        # å¦‚æœ context çš„å€¼ä¸ BaseContext çš„ç¡¬ç¼–ç é»˜è®¤å€¼ä¸€è‡´ï¼Œåˆ™å€¾å‘äºä½¿ç”¨åœºæ™¯å‚æ•°
        # è¿™é‡Œé‡‡ç”¨ç®€åŒ–é€»è¾‘ï¼šå¦‚æœ context ä¸­çš„å€¼ä¸ BaseContext é»˜è®¤å€¼ä¸åŒï¼Œè¯´æ˜æ˜¯ç”¨æˆ·æ˜¾å¼è®¾ç½®çš„ï¼Œä¼˜å…ˆä½¿ç”¨
        # å¦åˆ™ä½¿ç”¨åœºæ™¯å‚æ•°
        base_default = getattr(BaseContext(), name, None)
        
        if val is not None and val != base_default:
            return val
        return scenario_params.get(name, default_val)

    # å‡†å¤‡æ¨¡å‹ç»‘å®šå‚æ•°
    max_tokens = get_param("max_tokens", 8192)

    # é™åˆ¶ max_tokens é¿å…è¶…å‡ºæ¨¡å‹è¾“å‡ºé™åˆ¶
    if max_tokens > 16384:
        logger.warning(
            f"[{agent_name}] max_tokens {max_tokens} å¯èƒ½è¿‡å¤§ï¼Œè°ƒæ•´ä¸º 16384 ä»¥æé«˜æµå¼è¾“å‡ºç¨³å®šæ€§"
        )
        max_tokens = 16384

    bind_params = {
        "temperature": get_param("temperature", 0.7),
        "top_p": get_param("top_p", 0.9),
        "max_tokens": max_tokens,
    }

    # å¤„ç† extra_body (é’ˆå¯¹å…¶ä»–å¯èƒ½çš„å‚æ•°)
    # æ³¨æ„ï¼šæŸäº›å‚æ•°åœ¨æµå¼æ¨¡å¼ä¸‹å¯èƒ½å¯¼è‡´é—®é¢˜ï¼Œåªä¿ç•™å®‰å…¨çš„å‚æ•°
    extra_body = {}

    if extra_body:
        bind_params["extra_body"] = extra_body

    logger.bind(
        type="business_node",
        node="model_param_adjustment",
        scenario=scenario,
        model_params=bind_params
    ).info(
        "[{}] >>> åº”ç”¨åœºæ™¯å‚æ•° [{}]:\n"
        "  - temperature: {}\n"
        "  - top_p: {}\n"
        "  - max_tokens: {}\n"
        "  - extra_body: {}",
        agent_name, scenario,
        bind_params['temperature'],
        bind_params['top_p'],
        bind_params['max_tokens'],
        extra_body if extra_body else 'None'
    )

    # è·å–å½“å‰æ¨¡å‹å¹¶ç»‘å®šå‚æ•°
    current_model = request.model
    try:
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦æ”¯æŒ bind æ–¹æ³•
        if hasattr(current_model, "bind"):
            logger.debug("[{}] ç»‘å®šæ¨¡å‹å‚æ•°: {}", agent_name, bind_params)
            new_model = current_model.bind(**bind_params)
            request = request.override(model=new_model)
        else:
            logger.warning("[{}] æ¨¡å‹ä¸æ”¯æŒ bind æ–¹æ³•ï¼Œè·³è¿‡å‚æ•°ç»‘å®š", agent_name)
    except Exception as e:
        logger.bind(
            type="model_bind_error",
            agent=agent_name,
            params=bind_params
        ).exception("[{}] æ¨¡å‹å‚æ•°ç»‘å®šå¤±è´¥", agent_name)
        # ç»‘å®šå¤±è´¥æ—¶ä½¿ç”¨åŸæ¨¡å‹ç»§ç»­

    return await handler(request)


class DeliverableAgent(BaseAgent):
    """äº¤ä»˜ç‰©ç”Ÿæˆæ™ºèƒ½ä½“

    ä¸“ä¸šçš„æ–‡æ¡£å†™ä½œåŠ©æ‰‹ï¼Œå…·å¤‡å¤§çº²è§„åˆ’ã€ç« èŠ‚æ‰©å†™ã€é£æ ¼ç»Ÿä¸€ä¸è´¨é‡è‡ªæ£€èƒ½åŠ›ã€‚
    """
    name = "äº¤ä»˜ç‰©ç”Ÿæˆæ™ºèƒ½ä½“"
    description = "ä¸“ä¸šçš„æ–‡æ¡£å†™ä½œåŠ©æ‰‹ï¼Œå…·å¤‡å¤§çº²è§„åˆ’ã€ç« èŠ‚æ‰©å†™ã€é£æ ¼ç»Ÿä¸€ä¸è´¨é‡è‡ªæ£€èƒ½åŠ›ã€‚"
    capabilities = ["planning", "rag", "reflection", "consistency", "file_upload"]
    context_schema = DeliverableContext

    def __init__(self, **kwargs):
        super().__init__(**kwargs)

    async def get_graph(self, input_context=None, query: str = None, **kwargs):
        # æ³¨æ„ï¼šä¸ºäº†æ”¯æŒåŠ¨æ€åœºæ™¯åˆ‡æ¢ï¼Œè¿™é‡Œä¸ä½¿ç”¨ç¼“å­˜
        # æ¯æ¬¡ get_graph è°ƒç”¨éƒ½ä¼šæ ¹æ®å½“å‰çš„ input_context é‡æ–°åˆ›å»º graph
        # è¿™æ ·å¯ä»¥ç¡®ä¿ä¸åŒåœºæ™¯ä½¿ç”¨ä¸åŒçš„å‚æ•°é…ç½®

        # ä»ä¸Šä¸‹æ–‡è·å–æ¨¡å‹å‚æ•°é…ç½®ï¼ˆä¼ å…¥è¿è¡Œæ—¶ context ä»¥ä¾¿æ­£ç¡®è¯†åˆ«åœºæ™¯ï¼‰
        context = self.context_schema.from_file(
            module_name=self.module_name,
            input_context=input_context
        )

        # ğŸ†• å¢å¼ºä¸Šä¸‹æ–‡ï¼šå¦‚æœæä¾›äº† deliverableIdï¼Œä»æ•°æ®åº“åŒæ­¥æœ€æ–°çŠ¶æ€å’Œå¤§çº²
        if hasattr(context, "deliverableId") and context.deliverableId:
            try:
                from src.storage.db.manager import db_manager
                from src.storage.db.models import ProjectDeliverable
                from sqlalchemy import select

                async with db_manager.AsyncSession() as session:
                    deliverable_id = int(context.deliverableId)
                    query_stmt = select(ProjectDeliverable).where(ProjectDeliverable.id == deliverable_id)
                    result = await session.execute(query_stmt)
                    deliverable = result.scalar_one_or_none()
                    
                    if deliverable:
                        # åŒæ­¥çŠ¶æ€
                        context.status = deliverable.status
                        
                        # åŒæ­¥å¤§çº² (å¦‚æœ context ä¸­æ²¡æœ‰å¤§çº²ï¼Œæˆ–è€…éœ€è¦å¼ºåˆ¶åŒæ­¥)
                        if deliverable.extra_metadata and "outline" in deliverable.extra_metadata:
                            db_outline = deliverable.extra_metadata["outline"]
                            if not context.documentStructure and not context.existingOutline:
                                context.documentStructure = db_outline
                                context.existingOutline = db_outline
                                logger.debug(f"Synced {len(db_outline)} sections from DB for deliverable {deliverable_id}")
            except Exception as e:
                logger.warning(f"Failed to sync deliverable status from DB: {e}")

        # è‡ªåŠ¨æ£€æµ‹åœºæ™¯å¹¶è·å–åœºæ™¯å‚æ•°
        from .config import detect_scenario, get_scenario_params
        scenario = detect_scenario(context, query=query)
        scenario_params = get_scenario_params(scenario)

        # å‡†å¤‡æ¨¡å‹å‚æ•°ï¼šä¼˜å…ˆä½¿ç”¨åœºæ™¯å‚æ•°ï¼Œå¦‚æœ context ä¸­æ˜ç¡®æŒ‡å®šäº†å‚æ•°åˆ™è¦†ç›–
        model_params = {
            "temperature": getattr(context, "temperature", scenario_params["temperature"]),
            "top_p": getattr(context, "top_p", scenario_params["top_p"]),
            "max_tokens": getattr(context, "max_tokens", scenario_params.get("max_tokens", 4096)),
        }

        # æ‰“å°æ¨¡å‹å‚æ•°é…ç½®ï¼ˆä¾¿äºè°ƒè¯•ï¼‰
        logger.bind(
            type="business_node",
            node="agent_init",
            scenario=scenario,
            model_params=model_params
        ).info(
            "DeliverableAgent initialized:\n"
            "  - Scenario: {} ({})\n"
            "  - temperature: {}\n"
            "  - top_p: {}\n"
            "  - max_tokens: {}",
            scenario, scenario_params['description'],
            model_params['temperature'],
            model_params['top_p'],
            model_params['max_tokens']
        )

        # åŠ è½½å¸¦å‚æ•°çš„æ¨¡å‹
        model = load_chat_model(config.default_model, **model_params)

        # åˆ›å»ºåŠ¨æ€å·¥å…·ä¸­é—´ä»¶å®ä¾‹ï¼Œå¹¶ä¼ å…¥æ‰€æœ‰å¯ç”¨çš„ MCP æœåŠ¡å™¨åˆ—è¡¨
        from src.agents.common.mcp import MCP_SERVERS
        from src.agents.common.middlewares import (
            DynamicToolMiddleware,
            inject_attachment_context,
            RobustPatchToolCallsMiddleware,
            token_trimming_middleware,
        )
        from langchain.agents.middleware import ModelRetryMiddleware

        dynamic_tool_middleware = DynamicToolMiddleware(
            base_tools=get_tools(), mcp_servers=list(MCP_SERVERS.keys())
        )
        
        # é¢„åŠ è½½æ‰€æœ‰ MCP å·¥å…·å¹¶æ³¨å†Œåˆ° middleware.tools
        await dynamic_tool_middleware.initialize_mcp_tools()

        # åˆ›å»º DeliverableAgent
        graph = create_agent(
            model=model,
            tools=get_tools(),  # ä½¿ç”¨ tools.py ä¸­å®šä¹‰çš„å·¥å…·
            middleware=[
                deliverable_prompt_middleware,  # 1. æç¤ºè¯æ³¨å…¥ (ä¼˜å…ˆæ³¨å…¥ï¼Œç¡®ä¿ä¿®å‰ªå™¨èƒ½çœ‹åˆ°æœ€æ–°çš„ System Prompt)
                token_trimming_middleware,      # 2. æ¶ˆæ¯å†å²ä¿®å‰ª (ç½®äºæ³¨å…¥ä¹‹åï¼Œç¡®ä¿ä¿®å‰ªçš„æ˜¯æ—§å†å²ï¼Œå¹¶ä¿ç•™æœ€æ–°çš„ System Prompt)
                inject_attachment_context,      # 3. é™„ä»¶ä¸Šä¸‹æ–‡æ³¨å…¥
                context_based_model,            # 4. æ¨¡å‹é€‰æ‹©
                deliverable_model_middleware,   # 5. åŠ¨æ€å‚æ•°è°ƒæ•´
                dynamic_tool_middleware,        # 6. åŠ¨æ€å·¥å…·é€‰æ‹©
                RobustPatchToolCallsMiddleware(),  # 7. é²æ£’ä¿®å¤å·¥å…·è°ƒç”¨ JSON
                ModelRetryMiddleware(),         # 8. æ¨¡å‹é‡è¯•
                logging_middleware,             # 9. æ—¥å¿—è®°å½•
            ],
            checkpointer=await self._get_checkpointer(),
        )

        self.graph = graph
        return graph
